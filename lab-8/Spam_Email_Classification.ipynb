{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning - Text Classification\n",
    "\n",
    "Machine learning algorithms are used extensively in text analysis, and it is called as \"natural language processing\", where we try to make computers \"understand\" human language. Here, Scikit-learn package is used to implement machine learning algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "from math import log, sqrt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.metrics as metrics\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to text analysis\n",
    "\n",
    "As you might imagine, computers cannot read words like we do. Instead, we have to convert words to numbers. The best way is to use scikit-learn's \"CountVectorizer\" function. Before we go into text analysis, we will briefly cover how texts can be analyzed in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feel free to change this sentence\n",
    "sents = ['This is going to be the final Python lab for this semester.', 'We will be covering machine learning today',\n",
    "        'Machine learning is used frequently in spam email classification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = CountVectorizer(min_df=1, tokenizer=word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sents turned into sparse vector of word frequency counts\n",
    "sents_counts = vec.fit_transform(sents)\n",
    "# This shows the vocab dictionary which maps unique words to indexes\n",
    "vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the words are transformed to numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_counts.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the original sentences. Each sentence is shown as a long list of numbers. The number represents the count of that unique word in the sentence. If you have many unique words, sentences will have many 0s inside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While using the raw counts of words can be a useful method to classify data, we are often interested in words that appear often in a particular document, but not in many documents. For example, words such as \"Dear\", \"Hi\", \"and\" will appear frequently in emails, but it is highly likely that it will be used in both spam and usual email. Instead, we are interested in words that only appear in spam emails, such as \"free\", \"won\", \"prize\", etc.\n",
    "\n",
    "One of the most common methods to detect unique words is by using the \"term frequency-inverse document frequency\" (tf-idf). While I won't be explaining the details behind this algorithm, scikit-learn's \"TfidfTransformer\" transforms the sentence by using:\n",
    "\n",
    "$$ tfidf(w,d)=tf*log(\\frac{N+1}{N_W+1})+1 $$\n",
    "\n",
    "Here, $N$ is the number of documents in the training set, $N_W$ is the number of documents in the training set that the word $w$ appears in the document that you want to transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it will be difficult to comprehend this idea, we will be using this algorithm to the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_transformer = TfidfTransformer()\n",
    "sents_tfidf = tfidf_transformer.fit_transform(sents_counts)\n",
    "sents_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we were able to transform the three sentences that we had to a large list of numbers that indicate the important words. Computers are great at understanding numerical data, so we will be feeding this to our machine learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read spam email data to our notebook\n",
    "\n",
    "Now that we have finished a brief introduction to text analysis, we will start our analysis. At first, we will be reading the csv file that has emails inside."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the code in r'...' to your directory. The spam_email.csv file is in this Github repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'C:\\Users\\daiki\\Documents\\spam_email.csv', encoding = 'latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code drops the columns that are unnecessary to this analysis\n",
    "df.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1, inplace = True, errors='ignore')\n",
    "# This code renames the column name\n",
    "df.rename(columns = {'v1': 'labels', 'v2': 'message'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe with the correct column name. The details about this code should have been covered in previous lab sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data has 5,572 messages. The label \"ham\" shows the usual email, and \"spam\" is used to label spam email. We have 4,825 usual emails and 747 spam emails."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For binary classification data like this, we have to convert labels to numerical data. Here, we will make 0 as usual mail and 1 as spam email. Usually, you make 1 to be the label that you would like to analyze (spam email in this example)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = df['labels'].map({'ham': 0, 'spam': 1})\n",
    "df.drop(['labels'], axis = 1, inplace = True, errors='ignore')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, 0 represents usual email, and 1 represents spam email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we use machine learning algorithm for classification, it is better to visualize data. Here, we will use WordCloud algorithm, which is used to visualize word importance. The top figure shows the common words in spam email, and the bottom figure shows the common words in usual email.It seems that these two emails have different common words, so we would expect the machine learning algorithm to achieve high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_words = ' '.join(list(df[df['label'] == 1]['message']))\n",
    "spam_wc = WordCloud(width = 512,height = 512).generate(spam_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(spam_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_words = ' '.join(list(df[df['label'] == 0]['message']))\n",
    "ham_wc = WordCloud(width = 512,height = 512).generate(ham_words)\n",
    "plt.figure(figsize = (10, 8), facecolor = 'k')\n",
    "plt.imshow(ham_wc)\n",
    "plt.axis('off')\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform text data\n",
    "\n",
    "Now that we have a correct dataset, we will be converting the words to numbers by using the tf-idf algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, min_df represents the minimum count of word. Since we don't want to include words that are\n",
    "# expressed only once in the entire document, we set min_df to be 2\n",
    "email_vec = CountVectorizer(min_df=2, tokenizer=word_tokenize)\n",
    "count_data = email_vec.fit_transform(df.message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have 4,440 unique words in the document. 5572 represents the total sentences in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'dear' is found in the email, mapped to index 1294\n",
    "email_vec.vocabulary_.get('dear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'free' is found in the email, mapped to index 1732\n",
    "email_vec.vocabulary_.get('free')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert raw frequency counts into TF-IDF values\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "email_tfidf = tfidf_transformer.fit_transform(count_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original data looks like this. It seems as if there are no elements in the data, but this is how it should look. In each sentences, I would imagine that there is only around 10-20 words inside. Each element in this vector represents the tf-idf values. There are 4400 unique words in this document. It means that only 10-20 elements out of 4400 elements have a non-zero value. This is one of the issues in using conventional methods for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Implementation\n",
    "\n",
    "Now that we have our data ready, we will be implementing machine learning algorithm.\n",
    "\n",
    "The first step will be separting the data to train, validation, and the final test dataset. Train dataset will be used to train the machine learning algorithm, and the test dataset will be used to examine the accuracy of the final machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    email_tfidf, df.label, test_size = 0.20, random_state = 12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X represents the original data (sentences), and y represents the label. We have train, valid, and test for X and y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"We have {} sentences in train dataset\".format(y_train.shape[0]))\n",
    "print(\"We have {} sentences in test dataset\".format(y_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we will be using a machine learning model called \"Logistic Regression\". The detailed explanation of the logistic regression algorithm is written in the document that I sent you before.\n",
    "\n",
    "As a first step, let's use the default parameters in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression Model\n",
    "log = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "\n",
    "y_pred = log.predict(X_test)\n",
    "print(\"The accuracy of this model is: {}\".format(sklearn.metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an astonishing accuracy of 96.4%!!! Can we improve this model?\n",
    "\n",
    "We can select the optimal parameter in the model, and try to improve this machine learning prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid={'penalty':[\"l1\",\"l2\"],\n",
    "           'C':[0.001,0.01,0.1,1,10,100]}\n",
    "grid = GridSearchCV(LogisticRegression(solver='liblinear',max_iter=1000), param_grid = param_grid, cv=5)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"Best cross-validation accuracy: {:.2f}\".format(grid.best_score_))\n",
    "print(\"Best parameters: \", grid.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results by using a heatmap.\n",
    "\n",
    "This is the code for creating the heatmap. It requires some programming knowledge, so there is no need for you to try to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a function that tries to create a heatmap\n",
    "def heatmap(values, xlabel, ylabel, xticklabels, yticklabels, cmap=None,\n",
    "            vmin=None, vmax=None, ax=None, fmt=\"%0.2f\"):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    # plot the mean cross-validation scores\n",
    "    img = ax.pcolor(values, cmap=cmap, vmin=vmin, vmax=vmax)\n",
    "    img.update_scalarmappable()\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.set_xticks(np.arange(len(xticklabels)) + .5)\n",
    "    ax.set_yticks(np.arange(len(yticklabels)) + .5)\n",
    "    ax.set_xticklabels(xticklabels)\n",
    "    ax.set_yticklabels(yticklabels)\n",
    "    ax.set_aspect(1)\n",
    "\n",
    "    for p, color, value in zip(img.get_paths(), img.get_facecolors(),\n",
    "                               img.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.mean(color[:3]) > 0.5:\n",
    "            c = 'k'\n",
    "        else:\n",
    "            c = 'w'\n",
    "        ax.text(x, y, fmt % value, color=c, ha=\"center\", va=\"center\")\n",
    "    return img\n",
    "\n",
    "# extract scores from the grid search\n",
    "scores = grid.cv_results_['mean_test_score'].reshape(-1, 2).T\n",
    "\n",
    "#Visualize the results\n",
    "heatmap=heatmap(scores, xlabel=\"C\", ylabel=\"Penalty\", cmap=\"viridis\", fmt=\"%.3f\",xticklabels=param_grid['C'],\n",
    "                yticklabels=param_grid['penalty'])\n",
    "plt.colorbar(heatmap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the optimal parameter is \"C = 100\" with l2 penalty. Let's examine the accuracy of the new model with the updated parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Logistic Regression Model\n",
    "log = LogisticRegression(random_state=0).fit(X_train, y_train)\n",
    "y_pred = log.predict(X_test)\n",
    "\n",
    "# New Logistic Regression Model with the updated parameters\n",
    "log_new = LogisticRegression(random_state=0, C=100, penalty='l2').fit(X_train, y_train)\n",
    "y_pred_new = log_new.predict(X_test)\n",
    "\n",
    "print(\"The accuracy of the new model is: {}\".format(sklearn.metrics.accuracy_score(y_test, y_pred_new)))\n",
    "print(\"The accuracy of the default model is: {}\".format(sklearn.metrics.accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a slight increase in the percentage of prediction. In machine learning projects, it's ALWAYS vital to check the parameters of the model. Even though it's only a $2\\%$ increase in prediction, it is often important in large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation (Optional)\n",
    "\n",
    "We only examined the accuracy of the model. As a next step, we will try to use other methods to evaluate the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ROC curve and AUC\n",
    "\n",
    "An ROC curve (receiver operating characteristic curve) is a graph showing the performance of a classification model at all classification thresholds. The x-axis value shows the true positive rates, and the y-axis value shows the false positive rate. ROC curve is often used as a model evaluation tool in classification problem.\n",
    "\n",
    "AUC is the area under the ROC curve. The details about ROC curve and AUC is written in the given document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = log_new.predict_proba(X_test)\n",
    "preds = probs[:,1]\n",
    "fpr, tpr, threshold = metrics.roc_curve(y_test, preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, AUC is 0.98, and we see that the ROC curve is really close to 1. This looks like a very good algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Confusion Matrix\n",
    "\n",
    "It is often difficult to examine the accuracy of the model, as we can't understand the predicted values. Here, we will use a confusion matrix to evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot non-normalized confusion matrix\n",
    "class_names = [\"Usual\", \"Spam\"]\n",
    "matrix = metrics.confusion_matrix(y_test, y_pred_new)\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(matrix)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(class_names)))\n",
    "ax.set_yticks(np.arange(len(class_names)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        text = ax.text(j, i, matrix[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Confusion matrix, without normalization\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Normalized confusion matrix\n",
    "normalize= np.concatenate(([matrix[0]/np.sum(matrix[0])], [matrix[1]/np.sum(matrix[1])]))\n",
    "nor = np.around(normalize,3)\n",
    "fig, ax = plt.subplots()\n",
    "im = ax.imshow(nor)\n",
    "\n",
    "# We want to show all ticks...\n",
    "ax.set_xticks(np.arange(len(class_names)))\n",
    "ax.set_yticks(np.arange(len(class_names)))\n",
    "# ... and label them with the respective list entries\n",
    "ax.set_xticklabels(class_names)\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "# Rotate the tick labels and set their alignment.\n",
    "plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "         rotation_mode=\"anchor\")\n",
    "\n",
    "# Loop over data dimensions and create text annotations.\n",
    "for i in range(len(class_names)):\n",
    "    for j in range(len(class_names)):\n",
    "        text = ax.text(j, i, nor[i, j],\n",
    "                       ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "ax.set_title(\"Confusion matrix, with normalization\")\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, very few emails are classified incorrectly. Out of 965 usual emails, only 1 was misclassified, and out of 150 spam emails, only 19 were misclassified. Even though this machine learning algorithm achieves a high accuracy, it tends to classify emails as usual, instead of spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We are often interested in predicting email type based on the message. To achieve this, we can use the \"predict\" function in scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function defines a function for prediction.\n",
    "def pred_class(pred):\n",
    "    if pred[0]==0:\n",
    "        print(\"This email is not spam.\")\n",
    "    elif pred[0]==1:\n",
    "        print(\"This email is spam.\")\n",
    "    else:\n",
    "        raise ValueError(\"Invalid Class. The data should be binary.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a sample email\n",
    "email_new = ['Hi, all. We are planning to cover machine learning today.']\n",
    "\n",
    "email_new_counts = email_vec.transform(email_new)\n",
    "email_new_tfidf = tfidf_transformer.transform(email_new_counts)\n",
    "pred = log_new.predict(email_new_tfidf)\n",
    "pred_class(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a sample email\n",
    "email_new = ['Congratulations! You won a million dollars!']\n",
    "\n",
    "email_new_counts = email_vec.transform(email_new)\n",
    "email_new_tfidf = tfidf_transformer.transform(email_new_counts)\n",
    "pred = log_new.predict(email_new_tfidf)\n",
    "pred_class(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn. Copy and paste the code and write your own email, and see whether it is classified as spam or not spam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many emails use machine learning algorithms to classify spam emails. While this algorithm only focused on text data, it would be possible to include more data inside the machine learning algorithm, such as email address, image data in the email, etc. I hope you were able to get a brief understanding of how to implement machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Notes\n",
    "\n",
    "Statistics is my most favorite subject, and I hope you were able to understand how to use Python in statistics. The best way to improve coding is using Python in your dataset, and I believe that you might encounter many issues when you actually start your project in Python. Even though I won't be teaching you anymore, I will be more than welcome to help all of you after the course ends, and I will miss everyone.\n",
    "\n",
    "This was my first time to teach Python to students, but it was an extremely valuable experience for me. Thank you very much for joining this course, and I strongly hope that you were able to improve programming by taking this course."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
